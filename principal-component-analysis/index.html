<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<!--<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>-->
<!--<script type="text/x-mathjax-config">MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });</script>-->
<head>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<meta charset="utf-8">
<title>A few Notes on Principal Component Analysis &#8211; ML & Stats</title>
<meta name="description" content="Describe your website here.">
<meta name="keywords" content="">



<!-- Open Graph -->
<meta property="og:locale" content="en">
<meta property="og:type" content="article">
<meta property="og:title" content="A few Notes on Principal Component Analysis">
<meta property="og:description" content="Describe your website here.">
<meta property="og:url" content="https://MarkusThill.github.io/principal-component-analysis/">
<meta property="og:site_name" content="ML & Stats">
<meta property="og:image" content="https://MarkusThill.github.io/images/grandanse.jpg">






<link rel="canonical" href="https://MarkusThill.github.io/principal-component-analysis/">
<link href="https://MarkusThill.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="ML & Stats Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no"/>

<!-- For all browsers -->
<link rel="stylesheet" href="https://MarkusThill.github.io/assets/css/main.css">
<link rel="stylesheet" href="https://MarkusThill.github.io/assets/css/jquery.mmenu.all.css">
<link rel="stylesheet" href="https://MarkusThill.github.io/assets/css/jquery.floating-social-share.min.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script type="text/javascript" src="https://MarkusThill.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://MarkusThill.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://MarkusThill.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://MarkusThill.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://MarkusThill.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://MarkusThill.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://MarkusThill.github.io/images/apple-touch-icon-144x144-precomposed.png">




<!--<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">-->
<link rel="stylesheet" href="/fonts/cmun-serif.css"></link>

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A few Notes on Principal Component Analysis | ML &amp; Stats</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="A few Notes on Principal Component Analysis" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="\( \def\myX{\mathbf{X}} \def\mySigma{\mathbf{\Sigma}} \def\myT{\mathsf{T}} \) The main idea of principal component analysis (PCA) is to perform an orthogonal transformation of multi/high-dimensional numeric data so that the resulting data consists of linearly uncorrelated variables, which are called principal components. The first principle component (first variable) accounts for the most variability in the data (hence, the variance along this axis is the largest). The second principal component accounts for the highest possible variance in the data under the constraint that it is orthogonal to the first principal component and so on. Hence, the last principal component (which is orthogonal to all previous principal components) accounts for the least variability in the data. A common application of PCA is dimensionality reduction, where one omits the last principal components of the PCA-transformed data, since these variables usually only explain very little of the variance in the data and do not have significant predictive power. But PCA also has many other applications in statistics, exploratory data analysis (EDA), machine learning and neural networks. We will shortly look at one way how to derive PCA in this post." />
<meta property="og:description" content="\( \def\myX{\mathbf{X}} \def\mySigma{\mathbf{\Sigma}} \def\myT{\mathsf{T}} \) The main idea of principal component analysis (PCA) is to perform an orthogonal transformation of multi/high-dimensional numeric data so that the resulting data consists of linearly uncorrelated variables, which are called principal components. The first principle component (first variable) accounts for the most variability in the data (hence, the variance along this axis is the largest). The second principal component accounts for the highest possible variance in the data under the constraint that it is orthogonal to the first principal component and so on. Hence, the last principal component (which is orthogonal to all previous principal components) accounts for the least variability in the data. A common application of PCA is dimensionality reduction, where one omits the last principal components of the PCA-transformed data, since these variables usually only explain very little of the variance in the data and do not have significant predictive power. But PCA also has many other applications in statistics, exploratory data analysis (EDA), machine learning and neural networks. We will shortly look at one way how to derive PCA in this post." />
<link rel="canonical" href="https://markusthill.github.io/principal-component-analysis/" />
<meta property="og:url" content="https://markusthill.github.io/principal-component-analysis/" />
<meta property="og:site_name" content="ML &amp; Stats" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-17T11:48:39+02:00" />
<script type="application/ld+json">
{"description":"\\( \\def\\myX{\\mathbf{X}} \\def\\mySigma{\\mathbf{\\Sigma}} \\def\\myT{\\mathsf{T}} \\) The main idea of principal component analysis (PCA) is to perform an orthogonal transformation of multi/high-dimensional numeric data so that the resulting data consists of linearly uncorrelated variables, which are called principal components. The first principle component (first variable) accounts for the most variability in the data (hence, the variance along this axis is the largest). The second principal component accounts for the highest possible variance in the data under the constraint that it is orthogonal to the first principal component and so on. Hence, the last principal component (which is orthogonal to all previous principal components) accounts for the least variability in the data. A common application of PCA is dimensionality reduction, where one omits the last principal components of the PCA-transformed data, since these variables usually only explain very little of the variance in the data and do not have significant predictive power. But PCA also has many other applications in statistics, exploratory data analysis (EDA), machine learning and neural networks. We will shortly look at one way how to derive PCA in this post.","headline":"A few Notes on Principal Component Analysis","dateModified":"2018-10-17T11:48:39+02:00","datePublished":"2018-10-17T11:48:39+02:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://markusthill.github.io/principal-component-analysis/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://markusthill.github.io/images/logo.png"}},"url":"https://markusthill.github.io/principal-component-analysis/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="https://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->



<div class="header-menu header-menu-top">
    <ul class="header-item-container">
      <li class="header-item-title header-toggle "><a href="#menu"><h2><i class="fa fa-bars"></i></h2></a></li>
      <li class="header-item-title">
        <a href="https://MarkusThill.github.io/">
          
            <img class="logo" src="https://MarkusThill.github.io/images/logo.png" alt="ML & Stats">
          
          <a href="https://MarkusThill.github.io/" class="title"> ML & Stats</a>
        </a>
      </li>
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/posts"><h3>Posts</h3></a></li>
            
        
      
        
        

        
          <li class="header-item "><a href="https://MarkusThill.github.io/categories"><h3>Categories</h3></a>
            <ul class="header-submenu">
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Electronics">Electronics</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#ML">ML</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Math">Math</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Programming">Programming</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Stats">Stats</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Template">Template</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Vector Algebra">Vector Algebra</a></li>
              
            </ul>
          </li>
        
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/tags"><h3>Tags</h3></a></li>
            
        
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/markus"><h3>About</h3></a></li>
            
        
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/"><h3>Home</h3></a></li>
            
        
      
      <li class="header-item"><a href="https://MarkusThill.github.io/search"><h3><i class="fa fa-search"></i></h3></a></li>
    </ul>
  </div>
<div class="entry-header">
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>A few Notes on Principal Component Analysis</h1>
      
        <h2><span class="entry-date date published updated"><time datetime="2018-10-17T11:48:39+02:00">October 17, 2018</time></span></h2>
      

      
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
          Reading time ~5 minutes
        </p><!-- /.entry-reading-time -->
      
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->


<nav id="menu" style="display: none">
  <ul>
    
      
        <li><a href="https://MarkusThill.github.io/"><h3>Home</h3></a></li>
      
    
      
        <li><a href="https://MarkusThill.github.io/markus"><h3>About</h3></a></li>
      
    
      
        <li><a href="https://MarkusThill.github.io/tags"><h3>Tags</h3></a></li>
      
    
      
        <li><a href="https://MarkusThill.github.io/categories"><h3>Categories</h3></a>
          <ul>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Electronics">Electronics</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#ML">ML</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Math">Math</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Programming">Programming</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Stats">Stats</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Template">Template</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Vector Algebra">Vector Algebra</a></li>
            
          </ul>
        </li>
      
    
      
        <li><a href="https://MarkusThill.github.io/posts"><h3>Posts</h3></a></li>
      
    
  </ul>
</nav>




<div id="main" role="main">
  <article class="hentry">
    <div class="entry-content">
        
            <div class="entry-image-index">
              <img src="https://MarkusThill.github.io/images/grandanse.jpg" alt="A few Notes on Principal Component Analysis">
              
            </div>
        
      <h1 class="post-title entry-title">A few Notes on Principal Component Analysis</h1>
      <p>\( <br />
  \def\myX{\mathbf{X}}
  \def\mySigma{\mathbf{\Sigma}}
  \def\myT{\mathsf{T}}
\)</p>

<p>The main idea of principal component analysis (PCA) is to perform an orthogonal transformation of multi/high-dimensional numeric data so that the resulting data consists of linearly uncorrelated variables, which are called principal components. The first principle component (first variable) accounts for the most variability in the data (hence, the variance along this axis is the largest). The second principal component accounts for the highest possible variance in the data under the constraint that it is orthogonal to the first principal component and so on. Hence, the last principal component (which is orthogonal to all previous principal components) accounts for the least variability in the data. A common application of PCA is dimensionality reduction, where one omits the last principal components of the PCA-transformed data, since these variables usually only explain very little of the variance in the data and do not have significant predictive power.
But PCA also has many other applications in statistics, exploratory data analysis (EDA), machine learning and neural networks. We will shortly look at one way how to derive PCA in this post.</p>

<!--more-->
<h1 id="prerequisites">Prerequisites</h1>

<h2 id="law-of-cosines">Law of Cosines</h2>
<center>

<figure id="alt-and-id" class="centre-element max-500px-wide">
    <img src="/images/2018-10-17-principal-component-analysis/triangle.png" alt="alt and id" width="400px" />
    
      <figcaption><p style="font-family:'Luta';font-size:3;line-height:110%;"> <b>Figure 1:</b>  Derivation of the law of cosines for triangles. </p>
      </figcaption>
    
</figure>

</center>

<p>We can simply derive the law of cosines, by looking at the triangle above and writing down a few properties which we can observe:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  c^2 &= x^2 + h^2 & \label{eq:pythagoras}\\
  \cos \nu &= \frac{a-x}{b} &\Leftrightarrow& \ x = a - b \cos \nu \label{eq:cos} \\
  \sin \nu &= \frac{h}{b} &\Leftrightarrow& \ h = b \sin \nu \label{eq:sin} \\
\end{align} %]]></script>

<p>Now we simply insert Eq. \eqref{eq:cos} and \eqref{eq:sin} into Eq. \eqref{eq:pythagoras} and see where this leads us:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  & \ c^2 = (a - b \cos\nu)^2 + (b \sin \nu)^2 \\
  \Leftrightarrow& \ c^2 = a^2 - 2ab\cos\nu + b^2 \cos^2\nu + b^2\sin^2\nu \\
  \Leftrightarrow& \ c^2 = a^2 - 2ab\cos\nu + b^2 (\cos^2\nu + \sin^2\nu) \\
  \Leftrightarrow& \ c^2 = a^2  - 2ab\cos\nu + b^2 \\
  \Leftrightarrow& \ c^2 = a^2  + b^2 - 2ab\cos\nu \\
\end{align} %]]></script>

<h2 id="vector-inner-product">Vector Inner Product</h2>

<p>Remember that the vector inner product of two vectors <script type="math/tex">\vec{a}, \vec{b} \in \mathbb{R^n}</script> is defined as:</p>

<script type="math/tex; mode=display">\begin{align}
  \vec a \cdot \vec b = a_1 b_1 + a_2 b_2 + \ldots + a_n b_n
\end{align}</script>

<p>The inner product of a vector with itself is:
<script type="math/tex">% <![CDATA[
\begin{align}
  \vec a \cdot \vec a &= a_1 a_1 + a_2 a_2 + \ldots + a_n a_n \\
  &= a_1^2 + a_2^2 + \ldots + a_n^2 \\
  &= |\vec a|^2
\end{align} %]]></script></p>

<p>We can derive a slightly notation if we consider the law of cosines, as described in the section before:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{alignat}{2}
  && \ c^2 &= a^2  + b^2 - 2ab\cos\nu  \\
  &\Leftrightarrow& \ |\vec a - \vec b|^2 &= |\vec a|^2  + |\vec b|^2 - 2ab\cos\nu  \\
  &\Leftrightarrow& \ (\vec a - \vec b)(\vec a - \vec b) &= |\vec a|^2  + |\vec b|^2 - 2ab\cos\nu \\
  &\Leftrightarrow& \ |\vec a|^2 - 2 \vec a \vec b +  |\vec b|^2 &= |\vec a|^2  + |\vec b|^2 - 2ab\cos\nu \\
  &\Leftrightarrow& \  - 2 \vec a \vec b  &= - 2ab\cos\nu \\
  &\Leftrightarrow& \   \vec a \vec b &= ab\cos\nu \\
  &\Leftrightarrow& \   \vec a \vec b &= |\vec a| |\vec b|\cos\nu \\
\end{alignat} %]]></script>

<h2 id="vector-projection">Vector Projection</h2>
<p>Sometimes it is necessary to project a vector <script type="math/tex">\vec a</script> onto a vector <script type="math/tex">\vec b</script>. The figure below illustrates the projection step. We first try to find a line orthogonal to <script type="math/tex">\vec b</script> which connects to the end of vector <script type="math/tex">\vec a</script> (indicated by the dotted line). The resulting projection vector <script type="math/tex">\vec p_b</script> has the same direction as <script type="math/tex">\vec b</script> but has a length which is specified by the intersection of <script type="math/tex">\vec b</script> and the orthogonal dotted line.</p>

<center>

<figure id="alt-and-id" class="centre-element max-500px-wide">
    <img src="/images/2018-10-17-principal-component-analysis/projection.png" alt="alt and id" width="300px" />
    
      <figcaption><p style="font-family:'Luta';font-size:3;line-height:110%;"> <b>Figure 2:</b>  Projection of a vector <script type="math/tex">\vec a</script> onto a vector <script type="math/tex">\vec b</script>. The resulting vector is <script type="math/tex">\vec p_b</script>. </p>
      </figcaption>
    
</figure>

</center>

<p>In the following, we try to figure out a way to compute the project vector <script type="math/tex">\vec p_b</script>, if we have the vectors <script type="math/tex">\vec a</script> and <script type="math/tex">\vec b</script> given. Let us first compute the inner product of the <script type="math/tex">\vec a</script> and the unit vector in the direction of <script type="math/tex">\vec b</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  \vec a \cdot \frac{\vec b}{|\vec b|} &= |\vec a| \frac{|\vec b|}{|\vec b|} \cos \nu \\
  &= |\vec a| \cos \nu \\
  &= |\vec p_b|.
\end{align} %]]></script>

<p>With this little trick we have computed the length of the desired vector <script type="math/tex">\vec p_b</script>. Now we just have to multiply this length with the unit vector in direction <script type="math/tex">\vec b</script> and we are done:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  \vec p_b &= |\vec p_b| \frac{\vec b}{|\vec b|} \\
  &= \Bigg( \vec a \cdot \frac{\vec b}{|\vec b|} \Bigg) \cdot \frac{\vec b}{|\vec b|} \\
  &= \Bigg(  \frac{\vec a \cdot \vec b}{|\vec b| |\vec b|} \Bigg)\cdot \vec b \\
  &= \Bigg( \frac{\vec a \cdot \vec b}{|\vec b|^2} \Bigg) \cdot\vec b \\
  &= \Bigg(  \frac{\vec a \cdot \vec b}{\vec b \cdot \vec b} \Bigg) \cdot\vec b \\
\end{align} %]]></script>

<h1 id="deriving-the-principal-components-analysi">Deriving the Principal Components Analysi</h1>

<!--One has to note that PCA is sensitive to the scaling of the original variables of the data.-->

<ul>
  <li>Data X has to be centered (mean 0)</li>
  <li>Dimensions of e, X, x, mu, J</li>
  <li>Notation: we have <script type="math/tex">n</script> data points, arranged in a matrix <script type="math/tex">X</script> (rows for examples and columns as dimensions)</li>
</ul>

<p>One can say that PCA performs a orthogonal linear transformation which maps each data point to a new point in another coordinate system. In this new coordinate system the variance of the transformed data points happens to be the largest for the first axis, the second largest for the second axis, and so on. Hence, if our first axis is described by an vector <script type="math/tex">\vec e_1</script>, this axis would explain most of the variance in the data.
Since we want to find a vector <script type="math/tex">\vec e</script> which maximizes the variance along this vector, on which all data points <script type="math/tex">\vec x_i</script> are projected, we have an optimization problem for which we can write down the objective function in the form:</p>

<script type="math/tex; mode=display">\begin{align}
  J(\vec e) = \frac{1}{n}\sum_{i=1}^n ( \frac{1}{|\vec e|} \vec x_i^\myT \vec e - \mu_e)^2. \label{eq:objective1}
\end{align}</script>

<p>Note that <script type="math/tex">\mu_e</script> is the mean of all points in the projected space along the axis described by vector <script type="math/tex">\vec e</script>. We can compute the mean <script type="math/tex">\mu_e</script> along this axis <script type="math/tex">\vec e</script> as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  \mu_e &= \frac{1}{n}\sum_{i=1}^n  \frac{1}{|\vec e|} \vec x_i^\myT \vec e \\
    &= \frac{1}{n}\sum_{i=1}^n \frac{1}{|\vec e|} \Big(\sum_{j=1}^d x_{ij} e_j \Big) \\
    &=  \frac{1}{|\vec e|} \sum_{j=1}^d  \underbrace{\Big( \frac{1}{n}\sum_{i=1}^n   x_{ij}  \Big)}_{=0} e_j \\
    &= 0
\end{align} %]]></script>

<p>So, since the data along each dimension in the original data space is centered around the mean, also the mean in the projected space is zero for any arbitrary <script type="math/tex">\vec e</script>. This allows us to re-write Eq. \eqref{eq:objective1}
as:</p>

<script type="math/tex; mode=display">\begin{align}
  J(\vec e) = \frac{1}{n}\sum_{i=1}^n ( \frac{1}{|\vec e|} \vec x_i^\myT \vec e)^2. \label{eq:objective2}
\end{align}</script>

<p>In matrix notation, above expression simplifies to, based on rule TODO in article TODO:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  J(\vec e) &= \frac{1}{n} \Big(\myX \cdot \frac{\vec e}{|\vec e|}\Big)^\myT \cdot \Big(\myX \cdot \frac{\vec e}{|\vec e|} \Big) \label{eq:objective3}
\end{align} %]]></script>

<p>Since the vector <script type="math/tex">\vec e</script> is normalized by its norm, we can simplify above equation slightly more, by formulating the objective as a constrained optimization problem:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  \mbox{max } & \, \,J(\vec e) = \frac{1}{n} (\myX \cdot \vec e)^\myT \cdot (\myX \cdot \vec e) \\
  \mbox{s.t. } & \,  |\vec e| = 1 \\
\end{align} %]]></script>

<p>or</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  \mbox{max } & \,J(\vec e) = \frac{1}{n} (\myX \cdot \vec e)^\myT \cdot (\myX \cdot \vec e) \\
  \mbox{s.t. } & \, \,  \vec e^\myT \cdot \vec e - 1 = 0 \\
\end{align} %]]></script>

<p>This problem can be solved using the Lagrange multiplier method. The Lagrangian can therefore be written as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  \mathcal{L(\vec e, \lambda)} &= \frac{1}{n} (\myX \cdot \vec e)^\myT \cdot (\myX \cdot \vec e) - \lambda \cdot (\vec e^T \cdot \vec  - 1) \\
  &= \frac{1}{n} \vec e^\myT \myX^\myT \myX \vec e - \lambda \cdot (\vec e^T \cdot \vec e - 1) \\
  &= e^\myT \mySigma \vec e - \lambda \cdot (\vec e^T \cdot \vec e - 1) \\
\end{align} %]]></script>

<p>where we used the relation <script type="math/tex">\mySigma = \frac{1}{n}\myX^\myT \myX</script> (since the original data is centered around the means of each dimension).</p>

<p>Now we can compute the partial derivatives of <script type="math/tex">\mathcal{L(\vec e, \lambda)}</script> using the denominator layout and the rule TODO in TODO and set both gradients to zero:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  \frac{\partial }{\partial \vec e} \mathcal{L(\vec e, \lambda)} &= 2 \mySigma \vec e - 2\lambda\vec e = 0 \label{eq:gradient1}\\
    \frac{\partial }{\partial\lambda} \mathcal{L(\vec e, \lambda)} &= -\vec e^T \vec e + 1 = 0 \label{eq:gradient2}
\end{align} %]]></script>

<p>Eq. \eqref{eq:gradient2} leads us to the original constraint
<script type="math/tex">\begin{align}
     \vec e^T \vec e = 1
\end{align}</script></p>

<p>and Eq. \eqref{eq:gradient1} leads us to a more interesting equation
<script type="math/tex">% <![CDATA[
\begin{align}
  2 \mySigma \vec e - 2\lambda\vec e &= 0 \\
  \mySigma \vec e  &= \lambda\vec e, \label{eq:eigCandidates}
\end{align} %]]></script></p>

<p>which corresponds to finding the eigenvalues and eigenvectors of <script type="math/tex">\mySigma</script>. However, this results in a set of possible candidate solutions. But, since we want to maximize our objective <script type="math/tex">J(\vec e)</script>, we can simply insert all candidates from \eqref{eq:eigCandidates} into <script type="math/tex">J</script> and see, which candidate maximizes the value:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  \,J(\vec e) &= \frac{1}{n} (\myX \cdot \vec e)^\myT \cdot (\myX \cdot \vec e)\label{eq:maxJ1} \\
  &= \frac{1}{n} e^\myT \myX^\myT \myX \vec e \\
  &=  e^\myT \mySigma \vec e \\
  &= e^\myT \lambda\vec e \\
  &= \lambda |\vec e|^2 \\
  &= \lambda.\label{eq:maxJ2}
\end{align} %]]></script>

<p>Hence, the eigenvector with the largest corresponding eigenvalue <script type="math/tex">\lambda</script> is the solution we are looking for. Or in other words: The eigenvector <script type="math/tex">\vec e</script> with the largest eigenvalue <script type="math/tex">\lambda</script> for the covariance matrix <script type="math/tex">\mySigma</script> of our dataset <script type="math/tex">\myX</script> describes the new axis in our projected space, which (from all possible axes) maximizes the variance when all data points projected onto this axis. Furthermore, we also know how large this variance is: Remember that our original objective function <script type="math/tex">J(\vec e)</script> measures the variance for all data points projected onto <script type="math/tex">\vec e</script>.  If we look at Eq. \eqref{eq:maxJ1} – \eqref{eq:maxJ2}, this means that the variance actually has to be exactly the largest eigenvalue <script type="math/tex">\lambda</script>.</p>

<h2 id="todos">TODOs</h2>
<ul>
  <li>PCA in Bengios Book</li>
  <li>reduce dimensionality of problem and preserve as much of the structure of the data as possible. we represent the data with fewer variables</li>
  <li>feature selection a possibility , does not work always</li>
  <li>plot with xkcd</li>
  <li>relationship of pca and autoencoder</li>
  <li>svm of victor</li>
  <li>why largest variability? To preserve the relative distance between the points</li>
  <li>multiply vector with covariance matrix</li>
  <li>PCA-04: 06:00</li>
  <li>Example in 2d</li>
  <li>Optimization in the space of with dimension of e</li>
  <li>visualize the cost function + constraint for an example</li>
  <li>PCA on MNIST instead of eigen faces</li>
  <li>Derivation of Lagrange multipliers</li>
  <li>Eigenvectors are orthogonal</li>
  <li>What is a principal component</li>
  <li>How two find the second Principal Component</li>
</ul>

      <footer class="entry-meta">
        <span class="entry-tags"></span>
        
        <span class="author vcard"><span class="fn">Markus Thill</span></span>
        <div class="social-share">
  <ul class="socialcount socialcount-small inline-list">
    <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=https://MarkusThill.github.io/principal-component-analysis/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li>
    <li class="twitter"><a href="https://twitter.com/intent/tweet?text=https://MarkusThill.github.io/principal-component-analysis/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li>
    <li class="googleplus"><a href="https://plus.google.com/share?url=https://MarkusThill.github.io/principal-component-analysis/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li>
  </ul>
</div><!-- /.social-share -->

      </footer>
    </div><!-- /.entry-content -->
    <div class="read-more">
  <div class="read-more-header">
    <a href="https://MarkusThill.github.io/markus" class="read-more-btn">About the Author</a>
  </div><!-- /.read-more-header -->
  <div class="read-more-content author-info">
    <h3>Markus Thill</h3>
    <div class="author-container">
      <img class="author-img" src="https://MarkusThill.github.io/images/avatar.jpg" alt="Markus Thill" />
      <div class="author-bio">I studied computer engineering (B.Sc.) and Automation & IT (M.Eng.). Generally, I am interested in machine learning (ML) approaches (in the broadest sense), but particularly in the fields of time series analysis, anomaly detection, Reinforcement Learning (e.g. for board games), Deep Learning (DL) and incremental (on-line) learning procedures. </div>
    </div>
    <div class="author-share">
      <ul class="list-inline social-buttons">
        
          <li><a href="https://github.com/markusthill" target="_blank"><i class="fa fa-github fa-fw"></i></a></li>
        
          <li><a href="https://www.linkedin.com/in/markus-thill-a4991090/" target="_blank"><i class="fa fa-linkedin fa-fw"></i></a></li>
        
      </ul>
      
        <a aria-label="Follow @MarkusThill on GitHub" data-size="large" href="https://github.com/MarkusThill" class="github-button">Follow @MarkusThill</a>
      
      <br>
      
    </div>
  </div>
</div>

    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="https://MarkusThill.github.io/math/stats/the-sailors-problem/" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="https://MarkusThill.github.io/deriving-a-closed-form-solution-of-the-fibonacci-sequence/" title="Deriving a Closed-Form Solution of the Fibonacci Sequence using the Z-Transform">Deriving a Closed-Form Solution of the Fibonacci Sequence using the Z-Transform</a></h3>
      <p>The Fibonacci sequence might be one of the most famous sequences in the field of mathmatics and computer science. Already high school stu...&hellip; <a href="https://MarkusThill.github.io/deriving-a-closed-form-solution-of-the-fibonacci-sequence/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="https://MarkusThill.github.io/derivation-of-a-weighted-recursive-least-squares-estimator/" title="Derivation of a Weighted Recursive Linear Least Squares Estimator">Derivation of a Weighted Recursive Linear Least Squares Estimator</a></h4>
        <span>Published on May 05, 2019</span>
      </div><!-- /.list-item -->
    
      <div class="list-item">
        <h4><a href="https://MarkusThill.github.io/gaussian-distribution-with-a-diagonal-covariance-matrix/" title="Gaussian Distribution With a Diagonal Covariance Matrix">Gaussian Distribution With a Diagonal Covariance Matrix</a></h4>
        <span>Published on May 04, 2019</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript">window.jQuery || document.write('<script type="text/javascript" src="https://MarkusThill.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script type="text/javascript" src="https://MarkusThill.github.io/assets/js/scripts.min.js"></script>
<script type="text/javascript" async defer id="github-bjs" src="https://buttons.github.io/buttons.js"></script>
<script type="text/javascript">!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^https:/.test(d.location)?'https':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>



<!-- Asynchronous Google Analytics snippet -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-113918188-1', 'auto');
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>



    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'markusthill-github-io'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>




<script type="text/javascript">
    sharing();
</script>



<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 Markus Thill. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://github.com/aron-bordin/neo-hpstr-jekyll-theme" rel="nofollow">Neo-HPSTR Theme</a>.</span>

  </footer>
</div><!-- /.footer-wrapper -->

</body>
</html>
