<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<!--<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>-->
<!--<script type="text/x-mathjax-config">MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });</script>-->
<head>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<meta charset="utf-8">
<title>Online Estimation of Weighted Sample Mean and Coviarance Matrix &#8211; ML & Stats</title>
<meta name="description" content="Describe your website here.">
<meta name="keywords" content="Online, Gaussians, Covariances, Maximum Likelihood Estimation">



<!-- Open Graph -->
<meta property="og:locale" content="en">
<meta property="og:type" content="article">
<meta property="og:title" content="Online Estimation of Weighted Sample Mean and Coviarance Matrix">
<meta property="og:description" content="Describe your website here.">
<meta property="og:url" content="https://MarkusThill.github.io/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/">
<meta property="og:site_name" content="ML & Stats">
<meta property="og:image" content="https://MarkusThill.github.io/images/stats.jpg">






<link rel="canonical" href="https://MarkusThill.github.io/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/">
<link href="https://MarkusThill.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="ML & Stats Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no"/>

<!-- For all browsers -->
<link rel="stylesheet" href="https://MarkusThill.github.io/assets/css/main.css">
<link rel="stylesheet" href="https://MarkusThill.github.io/assets/css/jquery.mmenu.all.css">
<link rel="stylesheet" href="https://MarkusThill.github.io/assets/css/jquery.floating-social-share.min.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script type="text/javascript" src="https://MarkusThill.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://MarkusThill.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://MarkusThill.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://MarkusThill.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://MarkusThill.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://MarkusThill.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://MarkusThill.github.io/images/apple-touch-icon-144x144-precomposed.png">




<!--<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">-->
<link rel="stylesheet" href="/fonts/cmun-serif.css"></link>

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Online Estimation of Weighted Sample Mean and Coviarance Matrix | ML &amp; Stats</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Online Estimation of Weighted Sample Mean and Coviarance Matrix" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post is part 2 of a series of five articles: Online Maximum Likelihood Estimation of (multivariate) Gaussian Distributions Online Estimation of Weighted Sample Mean and Coviarance Matrix The Covariance of weighted Means Memory of the exponentially decaying Estimator for Mean and Covariance Matrix Online Estimation of the Inverse Covariance Matrix \( \def\myX{\mathbf{x}} \def\myM{\mathbf{M}} \def\myY{\mathbf{y}} \def\mySigma{\mathbf{\Sigma}} \def\myM{\mathbf{M}} \def\myT{\mathsf{T}} \) In certain cases, one might want to compute weighted a arithmetic mean and a sample covariance (matrix), if certain points of the sample should contribute more to the final mean (covariances) than others. For example, it could be reasonable to give larger weight to more recent data points, if the statistics (e.g. mean) of the underlying distribution change over time (concept drifts). In such cases we would want the estimator to track the changes in the data generating distribution and forget about older knowledge. But also other scenarios are thinkable, in which a weighting of the data points might be useful. In this section we want to define the weighted arithmetic mean and weighted sample covariance, explore a few properties related to these weighted statistics and then design an estimator for multivariate Gaussians which exhibits some sort of forgetting." />
<meta property="og:description" content="This post is part 2 of a series of five articles: Online Maximum Likelihood Estimation of (multivariate) Gaussian Distributions Online Estimation of Weighted Sample Mean and Coviarance Matrix The Covariance of weighted Means Memory of the exponentially decaying Estimator for Mean and Covariance Matrix Online Estimation of the Inverse Covariance Matrix \( \def\myX{\mathbf{x}} \def\myM{\mathbf{M}} \def\myY{\mathbf{y}} \def\mySigma{\mathbf{\Sigma}} \def\myM{\mathbf{M}} \def\myT{\mathsf{T}} \) In certain cases, one might want to compute weighted a arithmetic mean and a sample covariance (matrix), if certain points of the sample should contribute more to the final mean (covariances) than others. For example, it could be reasonable to give larger weight to more recent data points, if the statistics (e.g. mean) of the underlying distribution change over time (concept drifts). In such cases we would want the estimator to track the changes in the data generating distribution and forget about older knowledge. But also other scenarios are thinkable, in which a weighting of the data points might be useful. In this section we want to define the weighted arithmetic mean and weighted sample covariance, explore a few properties related to these weighted statistics and then design an estimator for multivariate Gaussians which exhibits some sort of forgetting." />
<link rel="canonical" href="https://markusthill.github.io/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/" />
<meta property="og:url" content="https://markusthill.github.io/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/" />
<meta property="og:site_name" content="ML &amp; Stats" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-01-13T21:05:00+01:00" />
<script type="application/ld+json">
{"description":"This post is part 2 of a series of five articles: Online Maximum Likelihood Estimation of (multivariate) Gaussian Distributions Online Estimation of Weighted Sample Mean and Coviarance Matrix The Covariance of weighted Means Memory of the exponentially decaying Estimator for Mean and Covariance Matrix Online Estimation of the Inverse Covariance Matrix \\( \\def\\myX{\\mathbf{x}} \\def\\myM{\\mathbf{M}} \\def\\myY{\\mathbf{y}} \\def\\mySigma{\\mathbf{\\Sigma}} \\def\\myM{\\mathbf{M}} \\def\\myT{\\mathsf{T}} \\) In certain cases, one might want to compute weighted a arithmetic mean and a sample covariance (matrix), if certain points of the sample should contribute more to the final mean (covariances) than others. For example, it could be reasonable to give larger weight to more recent data points, if the statistics (e.g. mean) of the underlying distribution change over time (concept drifts). In such cases we would want the estimator to track the changes in the data generating distribution and forget about older knowledge. But also other scenarios are thinkable, in which a weighting of the data points might be useful. In this section we want to define the weighted arithmetic mean and weighted sample covariance, explore a few properties related to these weighted statistics and then design an estimator for multivariate Gaussians which exhibits some sort of forgetting.","headline":"Online Estimation of Weighted Sample Mean and Coviarance Matrix","dateModified":"2018-01-13T21:05:00+01:00","datePublished":"2018-01-13T21:05:00+01:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://markusthill.github.io/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://markusthill.github.io/images/logo.png"}},"url":"https://markusthill.github.io/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="https://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->



<div class="header-menu header-menu-top">
    <ul class="header-item-container">
      <li class="header-item-title header-toggle "><a href="#menu"><h2><i class="fa fa-bars"></i></h2></a></li>
      <li class="header-item-title">
        <a href="https://MarkusThill.github.io/">
          
            <img class="logo" src="https://MarkusThill.github.io/images/logo.png" alt="ML & Stats">
          
          <a href="https://MarkusThill.github.io/" class="title"> ML & Stats</a>
        </a>
      </li>
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/posts"><h3>Posts</h3></a></li>
            
        
      
        
        

        
          <li class="header-item "><a href="https://MarkusThill.github.io/categories"><h3>Categories</h3></a>
            <ul class="header-submenu">
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Electronics">Electronics</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#ML">ML</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Math">Math</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Programming">Programming</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Stats">Stats</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Template">Template</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Vector Algebra">Vector Algebra</a></li>
              
            </ul>
          </li>
        
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/tags"><h3>Tags</h3></a></li>
            
        
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/markus"><h3>About</h3></a></li>
            
        
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/"><h3>Home</h3></a></li>
            
        
      
      <li class="header-item"><a href="https://MarkusThill.github.io/search"><h3><i class="fa fa-search"></i></h3></a></li>
    </ul>
  </div>
<div class="entry-header">
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Online Estimation of Weighted Sample Mean and Coviarance Matrix</h1>
      
        <h2><span class="entry-date date published updated"><time datetime="2018-01-13T21:05:00+01:00">January 13, 2018</time></span></h2>
      

      
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
          Reading time ~3 minutes
        </p><!-- /.entry-reading-time -->
      
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->


<nav id="menu" style="display: none">
  <ul>
    
      
        <li><a href="https://MarkusThill.github.io/"><h3>Home</h3></a></li>
      
    
      
        <li><a href="https://MarkusThill.github.io/markus"><h3>About</h3></a></li>
      
    
      
        <li><a href="https://MarkusThill.github.io/tags"><h3>Tags</h3></a></li>
      
    
      
        <li><a href="https://MarkusThill.github.io/categories"><h3>Categories</h3></a>
          <ul>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Electronics">Electronics</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#ML">ML</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Math">Math</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Programming">Programming</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Stats">Stats</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Template">Template</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Vector Algebra">Vector Algebra</a></li>
            
          </ul>
        </li>
      
    
      
        <li><a href="https://MarkusThill.github.io/posts"><h3>Posts</h3></a></li>
      
    
  </ul>
</nav>




<div id="main" role="main">
  <article class="hentry">
    <div class="entry-content">
        
            <div class="entry-image-index">
              <img src="https://MarkusThill.github.io/images/stats.jpg" alt="Online Estimation of Weighted Sample Mean and Coviarance Matrix">
              
            </div>
        
      <h1 class="post-title entry-title">Online Estimation of Weighted Sample Mean and Coviarance Matrix</h1>
      <p>This post is part 2 of a series of five articles:</p>
<ol>
  <li><a href="/math/stats/ml/online-estimation-of-gaussians/">Online Maximum Likelihood Estimation of (multivariate) Gaussian Distributions</a></li>
  <li><strong><a href="/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/">Online Estimation of Weighted Sample Mean and Coviarance Matrix</a></strong></li>
  <li><a href="/math/stats/ml/onthe-covariance-of-the-weighted-mean/">The Covariance of weighted Means</a></li>
  <li><a href="/math/stats/ml/memory-of-an-exponentially-weighted-estimator-of-the-arithmetic-mean-and-covariance-matrix/">Memory of the exponentially decaying Estimator for Mean and Covariance Matrix</a></li>
  <li><a href="/math/stats/ml/online-estimation-of-the-inverse-covariance-matrix/">Online Estimation of the Inverse Covariance Matrix</a></li>
</ol>

<p>\( <br />
  \def\myX{\mathbf{x}}
  \def\myM{\mathbf{M}}
  \def\myY{\mathbf{y}}
  \def\mySigma{\mathbf{\Sigma}}
  \def\myM{\mathbf{M}}
  \def\myT{\mathsf{T}}
\)</p>

<p>In certain cases, one might want to compute weighted a arithmetic mean and a sample covariance (matrix), if certain points of the sample should contribute more to the final mean (covariances) than others. For example, it could be reasonable to give larger weight to more recent data points, if the statistics (e.g. mean) of the underlying distribution change over time (concept drifts). In such cases we would want the estimator to track the changes in the data generating distribution and forget about older knowledge. But also other scenarios are thinkable, in which a weighting of the data points might be useful.
In this section we want to define the weighted arithmetic mean and weighted sample covariance, explore a few properties related to these weighted statistics and then design an estimator for multivariate Gaussians which exhibits some sort of forgetting.</p>

<!--more-->

<h2 id="weighted-arithmetic-mean-and-sample-covariance">Weighted Arithmetic Mean and Sample Covariance</h2>

<p>In general, the weighted arithmetic mean is defined quite straightforward as:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
	\bar\myX_n &= \frac{\sum_{i=1}^{n} w'_i \myX_i}{\sum_{i=1}^{n} w'_i}, \label{eq:meanWeighted} \\
\end{align} %]]></script> <!---_ --></p>

<p>where <script type="math/tex">\myX_i</script> is the i-th data point (vector) and <script type="math/tex">w'_i</script> is the (unnormalized) weight assigned to the corresponding data point. If the weights are normalized, then we get:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\bar\myX_n &= \sum_{i=1}^{n} w_i \myX_i, \label{eq:meanWeighted2} \\
\end{align} %]]></script>

<p>where the normalized weights are defined as:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
	w_i &= \frac{w'_i}{W_n} = \frac{w'_i}{\sum_{i=1}^{n} w'_i}, \label{eq:normWeights} \\
\end{align} %]]></script> <!---_ --></p>

<p>with</p>

<p><script type="math/tex">\begin{align}
	W_n = \sum_{i=1}^{n} w'_i. \\
\end{align}</script> <!---_ --></p>

<p>The special case with <script type="math/tex">w_i=\frac{1}{n}</script> results in the formulation of the conventional mean.</p>

<p>According to <a class="citation" href="#price1972extension">[1]</a>, the biased weighted covariance matrix can be written similarly as:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
	\mySigma &= \frac{\myM^{(n)}}{W_n}  = \frac{\sum_{i=1}^{n} {w'_i(\myX_i - \bar\myX)(\myX_i - \bar\myX)^\myT}}{\sum_{i=1}^{n} {w'_i}} \label{eq:SigmaWeighted}
\end{align} %]]></script> <!---_ -->
where <script type="math/tex">\myM^{(n)}</script> is the so called scatter Matrix, with</p>

<script type="math/tex; mode=display">\myM^{(n)} =  \sum_{i=1}^{n} {w'_i(\myX_i - \bar\myX)(\myX_i - \bar\myX)^\myT}.</script>

<p>If the weights are frequency weights (each weight represents the count of a data point), then an unbiased estimator is found to be <a class="citation" href="#price1972extension">[1]</a>:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
	\mySigma &= \frac{\myM^{(n)} }{W_n - 1}
\end{align} %]]></script> <!---_ --></p>

<p>Otherwise, the following unbiased estimator can be used <a class="citation" href="#price1972extension">[1]</a>:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
	\mySigma &= \frac{\myM^{(n)} }{W_n - W^{(2)}_n / W_n} \label{eq:UnbiasedCovariance}
\end{align} %]]></script> <!---_ --></p>

<p>where</p>

<p><script type="math/tex">\begin{align}
W^{(2)}_n = \sum_{i=1}^{n} (w'_i)^2. \label{eq:UnbiasedCovariance2}
\end{align}</script> <!---_ --></p>

<h2 id="online-estimation-of-the-weighted-mean-and-covariance">Online Estimation of the weighted Mean and Covariance</h2>

<p>A recursive definition of the weighted arithmetic mean can be easily found to be:</p>

<p><script type="math/tex">\begin{align}
	\bar\myX_n = \frac{ w'_n \myX_n + W_{n-1} \bar\myX_{n-1} }{W_n}. \\
\end{align}</script> <!---_ --></p>

<p>which can also be expressed as:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
	\bar\myX_n &= \frac{ w'_n \myX_n + W_{n-1} \bar\myX_{n-1} }{W_n} \\
      &= \frac{ w'_n \myX_n }{W_n} + \frac{ W_{n-1} \bar\myX_{n-1} }{W_n} \\
      &= \frac{ w'_n \myX_n }{W_n} + \frac{ W_{n} - w'_n  }{W_n} \bar\myX_{n-1} \\
      &= \frac{ w'_n \myX_n }{W_n} + \bar\myX_{n-1} - \frac{w'_n \bar\myX_{n-1} }{W_n}  \\
      &= \bar\myX_{n-1} + \frac{ w'_n  }{W_n} (\myX_n - \bar\myX_{n-1})  \\
      &= \bar\myX_{n-1} + \frac{ w'_n  }{W_n} \mathbf\Delta_n  \\
\end{align} %]]></script> <!---_ --></p>

<p>with</p>

<script type="math/tex; mode=display">\mathbf\Delta_n = \myX_n - \bar\myX_{n-1}.</script>

<p>Similarly to before, we can also attempt to derive an online version of the estimator for the weighted covariance matrix.
We start again with the scatter matrix:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
\myM^{(n)} &= \sum_{i=1}^{n} w'_i(\myX_i-\bar{\myX}_n)(\myX_i-\bar{\myX}_n)^\myT \label{eq:defScatterMatrix}\\
&= \sum_{i=1}^{n}  w'_i\big[ \myX_i \myX_i^\myT - \myX_i \bar{\myX}_n^\myT -  \bar{\myX}_n\myX_i^\myT+\bar{\myX}_n\bar{\myX}_n^\myT \big]\\
&= \sum_{i=1}^{n}  w'_i\big[ \myX_i \myX_i^\myT - 2\myX_i \bar{\myX}_n^\myT + \bar{\myX}_n\bar{\myX}_n^\myT \big]\\
&= \sum_{i=1}^{n}  w'_i\myX_i \myX_i^\myT - 2w'_i \Bigg( \sum_{i=1}^{n} \myX_i \Bigg) \bar{\myX}_n^\myT + \sum_{i=1}^{n} w'_i \bar{\myX}_n\bar{\myX}_n^\myT \\
&= \sum_{i=1}^{n}  w'_i \myX_i \myX_i^\myT - 2 W_n  \bar{\myX}_n \bar{\myX}_n^\myT + W_n \bar{\myX}_n\bar{\myX}_n^\myT \\
&= \sum_{i=1}^{n}  w'_i \myX_i \myX_i^\myT - W_n \bar{\myX}_n\bar{\myX}_n^\myT \\
\end{align} %]]></script> <!---_ --></p>

<p>The increment from time step <script type="math/tex">n-1</script> to time step <script type="math/tex">n</script> can be computed as follows:
<script type="math/tex">% <![CDATA[
\begin{align}
\mathbf\Delta \myM^{(n)} &= \myM^{(n)} - \myM^{(n-1)} \\
&= \sum_{i=1}^{n} w'_i \myX_i \myX_i^T - W_n \bar{\myX}_n\bar{\myX}_n^T - \sum_{i=1}^{n-1} w'_i \myX_i \myX_i^T + W_{n-1} \bar{\myX}_{n-1}\bar{\myX}_{n-1}^T\\
&= w'_n \myX_n \myX_n^T - W_n \bar{\myX}_n\bar{\myX}_n^T + W_{n-1} \bar{\myX}_{n-1}\bar{\myX}_{n-1}^T \label{eqref:DeltaMWeight}
\end{align} %]]></script></p>

<p>With the already known expression</p>

<script type="math/tex; mode=display">\begin{align}
	\bar{\myX}_n = \frac{w'_n \myX_n+W_{n-1}\bar{\myX}_{n-1}}{W_n}
	\label{eq:xmeanNew1}
\end{align}</script>

<p>and with a rearranged formulation of above equation</p>

<script type="math/tex; mode=display">\begin{align}
	\bar{\myX}_{n-1} = \frac{W_n\bar{\myX}_{n}-w'_n \myX_n}{W_{n-1}}
	\label{eq:xmeanOld1}
\end{align}</script>

<p>we can start simplifying Eq. \eqref{eqref:DeltaMWeight}. We insert Eq. \eqref{eq:xmeanNew1} and Eq. \eqref{eq:xmeanOld1} into \eqref{eqref:DeltaMWeight}:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
	\mathbf\Delta \myM^{(n)} &= \myM^{(n)} - \myM^{(n-1)} \\
&= w'_n \myX_n \myX_n^\mathsf{T} - W_n \bar{\mathbf x}_n\bar{\mathbf x}_n^\mathsf{T} + W_{n-1} \bar{\mathbf x}_{n-1}\bar{\mathbf x}_{n-1}^\mathsf{T} \\
&= w'_n \myX_n \myX_n^\mathsf{T} - W_n \Big(\frac{w'_n \mathbf x_n+W_{n-1}\bar{\mathbf x}_{n-1} }{W_n} \Big) \bar{\myX}_n^\mathsf{T} + W_{n-1} \bar{\myX}_{n-1} \Big( \frac{W_n\bar{\myX}_{n}- w'_n\myX_n}{W_{n-1}} \Big)^\mathsf{T} \\
&= w'_n \myX_n \myX_n^\mathsf{T} -  \Big(w'_n \mathbf x_n+W_{n-1}\bar{\mathbf x}_{n-1}  \Big) \bar{\myX}_n^\mathsf{T} +  \bar{\myX}_{n-1} \Big( W_n\bar{\myX}_{n}- w'_n\myX_n \Big)^\mathsf{T} \\
&= w'_n \myX_n \myX_n^\mathsf{T} -  w'_n \mathbf x_n \bar{\myX}_n^\mathsf{T} + W_{n-1}\bar{\myX}_{n-1} \bar{\myX}_n^\mathsf{T}  +       W_n \bar{\myX}_{n-1} \bar{\myX}_{n}^\mathsf{T} - w'_n\bar{\myX}_{n-1} \myX_n^\mathsf{T} \\
&= w'_n \myX_n \myX_n^\mathsf{T} -  w'_n \mathbf x_n \bar{\myX}_n^\mathsf{T} + (W_n - W_{n-1} )\bar{\myX}_{n-1} \bar{\myX}_n^T   - w'_n\bar{\myX}_{n-1} \myX_n^\mathsf{T} \\
&= w'_n \myX_n \myX_n^\mathsf{T} -  w'_n \mathbf x_n \bar{\myX}_n^\mathsf{T} + w'_n \bar{\myX}_{n-1} \bar{\myX}_n^\mathsf{T}   - w'_n\bar{\myX}_{n-1} \myX_n^\mathsf{T} \\
&=  \myX_n (w'_n \myX_n^\mathsf{T} -  w'_n \bar{\myX}_n^\mathsf{T})     +  \bar{\myX}_{n-1} (w'_n \bar{\myX}_n^\mathsf{T}   - w'_n \myX_n^\mathsf{T} ) \\
&=  \myX_n (w'_n \myX_n^\mathsf{T} -  w'_n \bar{\myX}_n^\mathsf{T})     -  \bar{\myX}_{n-1} (w'_n \myX_n^\mathsf{T} - w'_n \bar{\myX}_n^\mathsf{T} ) \\
&=  (\myX_n -  \bar{\myX}_{n-1} ) (w'_n \myX_n^\mathsf{T} -  w'_n \bar{\myX}_n^\mathsf{T})       \\
&=  (\myX_n -  \bar{\myX}_{n-1} ) (w'_n \myX_n -  w'_n \bar{\myX}_n)^\mathsf{T}       \\
&=  w'_n (\myX_n -  \bar{\myX}_{n-1} ) (\myX_n - \bar{\myX}_n)^\mathsf{T}       \\
&=  w'_n \mathbf\Delta_n (\myX_n - \bar{\myX}_n)^\mathsf{T}   \label{eq:SigmaWeightedOnline}    \\
\end{align} %]]></script> <!--_--></p>

<h2 id="online-estimation-with-exponentially-decaying-weights">Online Estimation with exponentially decaying Weights</h2>

<p>An interesting online algorithm can be retrieved, if we use a particular set of weights, where each weight is</p>

<p><script type="math/tex">\begin{align}
  w'_i = \lambda^{(n-i)}.
  \end{align}</script> <!--_---></p>

<p>with <script type="math/tex">\lambda \in [0,1]</script>. With such a weighting, the most recent example will be weighted with <script type="math/tex">w'_n=1</script>, the second last will have <script type="math/tex">w'_{n-1}=\lambda</script> and the older examples will fade away exponentially, if <script type="math/tex">% <![CDATA[
\lambda<1 %]]></script>. The normalization factor <script type="math/tex">W_n</script> can be computed with:</p>

<script type="math/tex; mode=display">\begin{align}
W_n = \sum_{i=1}^n w'_i = \sum_{i=1}^n \lambda^{(n-i)} = \frac{1-\lambda^n}{1-\lambda}. \label{eq:NormalizationFactorDecaying}
\end{align}</script>

<p>However, note that the weightings <script type="math/tex">w'_{n-1}, w'_{n-2}, \ldots, w'_1</script> of all previous examples <script type="math/tex">\myX_{n-1}, \myX_{n-2}, \ldots, \myX_1</script>, as well as the normalization factor <script type="math/tex">W_n</script>, have to be adjusted if a new example <script type="math/tex">\myX_n</script> arrives. In this case, each weight <script type="math/tex">% <![CDATA[
\{w'_i \vert i<n \} %]]></script> has to be multiplied with <script type="math/tex">\lambda</script> and the weight of the new example is set to <script type="math/tex">w'_n = 1</script>. <!---_-->
How does this change the estimation of the mean and the covariance matrix? Typically, if all weights are changed, one would have to re-compute both statistics from scratch. However, if we look at Eq. \eqref{eq:meanWeighted} and Eq. \eqref{eq:SigmaWeightedOnline}, it is easy to see that in this particular case, this it not necessary, as shown in the following explications:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
	\bar\myX_n &= \frac{\sum_{i=1}^{n-1} (\lambda w'_i) \myX_i + \myX_n}{\sum_{i=1}^{n-1} (\lambda w'_i) + 1} \\
   &= \frac{\lambda\sum_{i=1}^{n-1}  w'_i \myX_i + \myX_n}{\lambda\sum_{i=1}^{n-1}  w'_i + 1} \\
   &= \frac{\lambda W_{n-1} \bar\myX_{n-1}  + \myX_n}{\lambda W_{n-1} + 1} \label{eq:weightedDecayingMean} \\
\end{align} %]]></script> <!---_ --></p>

<p>Let us set, as described before:</p>

<script type="math/tex; mode=display">\begin{align}
  W_n = \lambda W_{n-1} + 1 \label{eq:NormalaziationRecursive}
\end{align}</script>

<p>and then continue simplifying \eqref{eq:weightedDecayingMean}:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
	\bar\myX_n
   &= \frac{\lambda W_{n-1} \bar\myX_{n-1}  + \myX_n}{\lambda W_{n-1} + 1}  \\
   &= \frac{\lambda \frac{W_n - 1}{\lambda} \bar\myX_{n-1}  + \myX_n}{W_n} \\
   &= \frac{ (W_n - 1) \bar\myX_{n-1}  + \myX_n}{W_n} \\
   &= \bar\myX_{n-1} - \frac{\bar\myX_{n-1}}{W_n} + \frac{\myX_n}{W_n} \\
   &= \bar\myX_{n-1} + \frac{\myX_n - \bar\myX_{n-1}}{W_n} \\
   &= \bar\myX_{n-1} + \frac{\mathbf\Delta_n}{W_n}. \label{eq:weightedDecayingMeanSimpl} \\
\end{align} %]]></script> <!---_ --></p>

<p>As we can see in Eq. \eqref{eq:weightedDecayingMeanSimpl}, we can achieve a recursive form when using exponentially decaying weights and it is not necessary to re-compute the weighted mean from scratch. We can obtain a similar result for the covariance matrix starting with Eq. \eqref{eq:SigmaWeightedOnline}:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \myM^{(1)} &= 1\cdot \mathbf\Delta_1(\myX_1 - \bar\myX_1)^\mathsf{T}  \\
    \myM^{(2)} &= \lambda \cdot \mathbf\Delta_1(\myX_1 - \bar\myX_1)^\mathsf{T}  + 1 \cdot \mathbf\Delta_2(\myX_2 - \bar\myX_2)^\mathsf{T} = \lambda \myM^{(1)} + 1 \cdot \mathbf\Delta_2(\myX_2 - \bar\myX_2)^\mathsf{T} \\
    \myM^{(3)} &= \lambda^2 \cdot \mathbf\Delta_1(\myX_1 - \bar\myX_1)^\mathsf{T}  + \lambda \cdot \mathbf\Delta_2(\myX_2 - \bar\myX_2)^\mathsf{T} + 1\cdot \mathbf\Delta_3(\myX_3 - \bar\myX_3)^\mathsf{T} = \lambda \myM^{(2)} + 1 \cdot \mathbf\Delta_3(\myX_3 - \bar\myX_3)^\mathsf{T} \\
    \myM^{(n)} &= \lambda^{n-1} \cdot \mathbf\Delta_1(\myX_1 - \bar\myX_1)^\mathsf{T}  + \lambda^{n-2} \cdot \mathbf\Delta_2(\myX_2 - \bar\myX_2)^\mathsf{T} + \ldots + \mathbf\Delta_n(\myX_n - \bar\myX_n)^\mathsf{T} \\
    &= \lambda \myM^{(n-1)} + \mathbf\Delta_n(\myX_n - \bar\myX_n)^\mathsf{T}, \\
  \end{align} %]]></script>

<p>where the recursive form of the normalization factor <script type="math/tex">W_n</script> is then also the same as in Eq. \eqref{eq:NormalaziationRecursive}. The normalization factor for the unbiased estimator, as described in Eq. \eqref{eq:UnbiasedCovariance} – \eqref{eq:UnbiasedCovariance2}, can be computed similarly.</p>

<!---$$
\begin{align}
	\mySigma_n &= \frac{\sum_{i=1}^{n-1} {\lambda w'_i (\myX_i - \bar\myX_n)(\myX_i - \bar\myX_n)^\myT} + (\myX_n - \bar\myX_n)(\myX_n - \bar\myX_n)^\myT}{\sum_{i=1}^{n-1} {\lambda w'_i} + 1} \\
  &= \frac{\lambda \sum_{i=1}^{n-1} { w'_i (\myX_i - \bar\myX_n)(\myX_i - \bar\myX_n)^\myT} + (\myX_n - \bar\myX_n)(\myX_n - \bar\myX_n)^\myT} {W_n} \\
  &= \frac{\lambda \myM^{(n-1)} + (\myX_n - \bar\myX_n)(\myX_n - \bar\myX_n)^\myT} {W_n} \\
\end{align}
$$ _ -->

<h2 id="summary">Summary</h2>
<p>In summary, we can write down the following rules for the recursive (iterative) estimation of the mean vector and the covariance matrix, which should be processed in the given order:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
 W_n &= \lambda W_{n-1} + 1 \\
 \mathbf\Delta_n &= \myX_n - \bar\myX_{n-1}  \\
 \bar\myX_{n} &= \bar\myX_{n-1} + \frac{\mathbf\Delta_n}{W_n} \\
 \myM^{(n)} &= \lambda \myM^{(n-1)} + \mathbf\Delta_n(\myX_n - \bar\myX_n)^\mathsf{T} \\
 \mySigma_n &= \frac{1}{W_n} \myM^{(n)}
\end{align} %]]></script>

<p>This estimator has a memory comparable to an ordinary estimator, which approximately takes into account only the last
<script type="math/tex">n_{mem} \approx \frac{1-\lambda^2}{(1-\lambda)^2} = \frac{1+\lambda}{1-\lambda}</script>
data points, as we will describe <a href="/math/stats/ml/memory-of-an-exponentially-weighted-estimator-of-the-arithmetic-mean-and-covariance-matrix/">in another article</a>.</p>

<!--- TODO: ## Online Estimation with other exponentially decaying Scheme -->

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="price1972extension">G. R. Price, “Extension of covariance selection mathematics,” <i>Annals of human genetics</i>, vol. 35, no. 4, pp. 485–490, 1972.</span></li></ol>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="https://MarkusThill.github.io/tags#Online" title="Pages tagged Online" class="tag"><span class="term">Online</span></a><a href="https://MarkusThill.github.io/tags#Gaussians" title="Pages tagged Gaussians" class="tag"><span class="term">Gaussians</span></a><a href="https://MarkusThill.github.io/tags#Covariances" title="Pages tagged Covariances" class="tag"><span class="term">Covariances</span></a><a href="https://MarkusThill.github.io/tags#Maximum Likelihood Estimation" title="Pages tagged Maximum Likelihood Estimation" class="tag"><span class="term">Maximum Likelihood Estimation</span></a></span>
        
        <span class="author vcard"><span class="fn">Markus Thill</span></span>
        <div class="social-share">
  <ul class="socialcount socialcount-small inline-list">
    <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=https://MarkusThill.github.io/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li>
    <li class="twitter"><a href="https://twitter.com/intent/tweet?text=https://MarkusThill.github.io/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li>
    <li class="googleplus"><a href="https://plus.google.com/share?url=https://MarkusThill.github.io/math/stats/ml/online-estimation-of-weighted-sample-mean-and-coviarance-matrix/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li>
  </ul>
</div><!-- /.social-share -->

      </footer>
    </div><!-- /.entry-content -->
    <div class="read-more">
  <div class="read-more-header">
    <a href="https://MarkusThill.github.io/markus" class="read-more-btn">About the Author</a>
  </div><!-- /.read-more-header -->
  <div class="read-more-content author-info">
    <h3>Markus Thill</h3>
    <div class="author-container">
      <img class="author-img" src="https://MarkusThill.github.io/images/avatar.jpg" alt="Markus Thill" />
      <div class="author-bio">I studied computer engineering (B.Sc.) and Automation & IT (M.Eng.). Generally, I am interested in machine learning (ML) approaches (in the broadest sense), but particularly in the fields of time series analysis, anomaly detection, Reinforcement Learning (e.g. for board games), Deep Learning (DL) and incremental (on-line) learning procedures. </div>
    </div>
    <div class="author-share">
      <ul class="list-inline social-buttons">
        
          <li><a href="https://github.com/markusthill" target="_blank"><i class="fa fa-github fa-fw"></i></a></li>
        
          <li><a href="https://www.linkedin.com/in/markus-thill-a4991090/" target="_blank"><i class="fa fa-linkedin fa-fw"></i></a></li>
        
      </ul>
      
        <a aria-label="Follow @MarkusThill on GitHub" data-size="large" href="https://github.com/MarkusThill" class="github-button">Follow @MarkusThill</a>
      
      <br>
      
    </div>
  </div>
</div>

    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="https://MarkusThill.github.io/math/stats/ml/online-estimation-of-gaussians/" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="https://MarkusThill.github.io/deriving-a-closed-form-solution-of-the-fibonacci-sequence/" title="Deriving a Closed-Form Solution of the Fibonacci Sequence using the Z-Transform">Deriving a Closed-Form Solution of the Fibonacci Sequence using the Z-Transform</a></h3>
      <p>The Fibonacci sequence might be one of the most famous sequences in the field of mathmatics and computer science. Already high school stu...&hellip; <a href="https://MarkusThill.github.io/deriving-a-closed-form-solution-of-the-fibonacci-sequence/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="https://MarkusThill.github.io/derivation-of-a-weighted-recursive-least-squares-estimator/" title="Derivation of a Weighted Recursive Linear Least Squares Estimator">Derivation of a Weighted Recursive Linear Least Squares Estimator</a></h4>
        <span>Published on May 05, 2019</span>
      </div><!-- /.list-item -->
    
      <div class="list-item">
        <h4><a href="https://MarkusThill.github.io/gaussian-distribution-with-a-diagonal-covariance-matrix/" title="Gaussian Distribution With a Diagonal Covariance Matrix">Gaussian Distribution With a Diagonal Covariance Matrix</a></h4>
        <span>Published on May 04, 2019</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript">window.jQuery || document.write('<script type="text/javascript" src="https://MarkusThill.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script type="text/javascript" src="https://MarkusThill.github.io/assets/js/scripts.min.js"></script>
<script type="text/javascript" async defer id="github-bjs" src="https://buttons.github.io/buttons.js"></script>
<script type="text/javascript">!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^https:/.test(d.location)?'https':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>



<!-- Asynchronous Google Analytics snippet -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-113918188-1', 'auto');
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>



    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'markusthill-github-io'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>




<script type="text/javascript">
    sharing();
</script>



<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 Markus Thill. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://github.com/aron-bordin/neo-hpstr-jekyll-theme" rel="nofollow">Neo-HPSTR Theme</a>.</span>

  </footer>
</div><!-- /.footer-wrapper -->

</body>
</html>
