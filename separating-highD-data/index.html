<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<!--<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>-->
<!--<script type="text/x-mathjax-config">MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });</script>-->
<head>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<meta charset="utf-8">
<title>Separating High-Dimensional Data with given Points for the Decision Boundary &#8211; ML & Stats</title>
<meta name="description" content="Recently, someone gave me a strange problem, which seemed to be quite trivial on the first glance (and actually is. However, it took me a little time to put my rusty knowledge of vector and matrix algebra together) We have quite a few of n-dimensional data-points (exact knowledge of the actual data is not required here), which belong to one of two classes. For a few cases we know the class, but for many cases we do not. This might appear like a typical classification task, if it was not for one additional curiosity. we actually already know quite some data-points which lie on (or very close to) the decision boundary. This turns our original problem somehow into a regression problem, since we can estimate a hypersurface that fits the decision boundary. You also have to think about some things in this case as well, but in this blog post we want to look at another approach.">
<meta name="keywords" content="Classification, Multidimensionality">



<!-- Open Graph -->
<meta property="og:locale" content="en">
<meta property="og:type" content="article">
<meta property="og:title" content="Separating High-Dimensional Data with given Points for the Decision Boundary">
<meta property="og:description" content="Recently, someone gave me a strange problem, which seemed to be quite trivial on the first glance (and actually is. However, it took me a little time to put my rusty knowledge of vector and matrix algebra together) We have quite a few of n-dimensional data-points (exact knowledge of the actual data is not required here), which belong to one of two classes. For a few cases we know the class, but for many cases we do not. This might appear like a typical classification task, if it was not for one additional curiosity. we actually already know quite some data-points which lie on (or very close to) the decision boundary. This turns our original problem somehow into a regression problem, since we can estimate a hypersurface that fits the decision boundary. You also have to think about some things in this case as well, but in this blog post we want to look at another approach.">
<meta property="og:url" content="https://MarkusThill.github.io/separating-highD-data/">
<meta property="og:site_name" content="ML & Stats">
<meta property="og:image" content="https://MarkusThill.github.io/images/2254s.jpg">






<link rel="canonical" href="https://MarkusThill.github.io/separating-highD-data/">
<link href="https://MarkusThill.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="ML & Stats Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no"/>

<!-- For all browsers -->
<link rel="stylesheet" href="https://MarkusThill.github.io/assets/css/main.css">
<link rel="stylesheet" href="https://MarkusThill.github.io/assets/css/jquery.mmenu.all.css">
<link rel="stylesheet" href="https://MarkusThill.github.io/assets/css/jquery.floating-social-share.min.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script type="text/javascript" src="https://MarkusThill.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://MarkusThill.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://MarkusThill.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://MarkusThill.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://MarkusThill.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://MarkusThill.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://MarkusThill.github.io/images/apple-touch-icon-144x144-precomposed.png">




<!--<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">-->
<link rel="stylesheet" href="/fonts/cmun-serif.css"></link>

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Separating High-Dimensional Data with given Points for the Decision Boundary | ML &amp; Stats</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Separating High-Dimensional Data with given Points for the Decision Boundary" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Recently, someone gave me a strange problem, which seemed to be quite trivial on the first glance (and actually is. However, it took me a little time to put my rusty knowledge of vector and matrix algebra together) We have quite a few of n-dimensional data-points (exact knowledge of the actual data is not required here), which belong to one of two classes. For a few cases we know the class, but for many cases we do not. This might appear like a typical classification task, if it was not for one additional curiosity. we actually already know quite some data-points which lie on (or very close to) the decision boundary. This turns our original problem somehow into a regression problem, since we can estimate a hypersurface that fits the decision boundary. You also have to think about some things in this case as well, but in this blog post we want to look at another approach." />
<meta property="og:description" content="Recently, someone gave me a strange problem, which seemed to be quite trivial on the first glance (and actually is. However, it took me a little time to put my rusty knowledge of vector and matrix algebra together) We have quite a few of n-dimensional data-points (exact knowledge of the actual data is not required here), which belong to one of two classes. For a few cases we know the class, but for many cases we do not. This might appear like a typical classification task, if it was not for one additional curiosity. we actually already know quite some data-points which lie on (or very close to) the decision boundary. This turns our original problem somehow into a regression problem, since we can estimate a hypersurface that fits the decision boundary. You also have to think about some things in this case as well, but in this blog post we want to look at another approach." />
<link rel="canonical" href="https://markusthill.github.io/separating-highD-data/" />
<meta property="og:url" content="https://markusthill.github.io/separating-highD-data/" />
<meta property="og:site_name" content="ML &amp; Stats" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-10-06T20:42:49+02:00" />
<script type="application/ld+json">
{"description":"Recently, someone gave me a strange problem, which seemed to be quite trivial on the first glance (and actually is. However, it took me a little time to put my rusty knowledge of vector and matrix algebra together) We have quite a few of n-dimensional data-points (exact knowledge of the actual data is not required here), which belong to one of two classes. For a few cases we know the class, but for many cases we do not. This might appear like a typical classification task, if it was not for one additional curiosity. we actually already know quite some data-points which lie on (or very close to) the decision boundary. This turns our original problem somehow into a regression problem, since we can estimate a hypersurface that fits the decision boundary. You also have to think about some things in this case as well, but in this blog post we want to look at another approach.","headline":"Separating High-Dimensional Data with given Points for the Decision Boundary","dateModified":"2017-10-06T20:42:49+02:00","datePublished":"2017-10-06T20:42:49+02:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://markusthill.github.io/separating-highD-data/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://markusthill.github.io/images/logo.png"}},"url":"https://markusthill.github.io/separating-highD-data/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="https://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->



<div class="header-menu header-menu-top">
    <ul class="header-item-container">
      <li class="header-item-title header-toggle "><a href="#menu"><h2><i class="fa fa-bars"></i></h2></a></li>
      <li class="header-item-title">
        <a href="https://MarkusThill.github.io/">
          
            <img class="logo" src="https://MarkusThill.github.io/images/logo.png" alt="ML & Stats">
          
          <a href="https://MarkusThill.github.io/" class="title"> ML & Stats</a>
        </a>
      </li>
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/posts"><h3>Posts</h3></a></li>
            
        
      
        
        

        
          <li class="header-item "><a href="https://MarkusThill.github.io/categories"><h3>Categories</h3></a>
            <ul class="header-submenu">
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Electronics">Electronics</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#ML">ML</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Math">Math</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Programming">Programming</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Stats">Stats</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Template">Template</a></li>
              
                
                  <li class="sub-item"><a href="https://MarkusThill.github.io/categories/#Vector Algebra">Vector Algebra</a></li>
              
            </ul>
          </li>
        
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/tags"><h3>Tags</h3></a></li>
            
        
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/markus"><h3>About</h3></a></li>
            
        
      
        
        

        
            
                <li class="header-item "><a href="https://MarkusThill.github.io/"><h3>Home</h3></a></li>
            
        
      
      <li class="header-item"><a href="https://MarkusThill.github.io/search"><h3><i class="fa fa-search"></i></h3></a></li>
    </ul>
  </div>
<div class="entry-header">
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Separating High-Dimensional Data with given Points for the Decision Boundary</h1>
      
        <h2><span class="entry-date date published updated"><time datetime="2017-10-06T20:42:49+02:00">October 06, 2017</time></span></h2>
      

      
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
          Reading time ~6 minutes
        </p><!-- /.entry-reading-time -->
      
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->


<nav id="menu" style="display: none">
  <ul>
    
      
        <li><a href="https://MarkusThill.github.io/"><h3>Home</h3></a></li>
      
    
      
        <li><a href="https://MarkusThill.github.io/markus"><h3>About</h3></a></li>
      
    
      
        <li><a href="https://MarkusThill.github.io/tags"><h3>Tags</h3></a></li>
      
    
      
        <li><a href="https://MarkusThill.github.io/categories"><h3>Categories</h3></a>
          <ul>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Electronics">Electronics</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#ML">ML</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Math">Math</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Programming">Programming</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Stats">Stats</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Template">Template</a></li>
            
              
                <li><a href="https://MarkusThill.github.io/categories/#Vector Algebra">Vector Algebra</a></li>
            
          </ul>
        </li>
      
    
      
        <li><a href="https://MarkusThill.github.io/posts"><h3>Posts</h3></a></li>
      
    
  </ul>
</nav>




<div id="main" role="main">
  <article class="hentry">
    <div class="entry-content">
        
            <div class="entry-image-index">
              <img src="https://MarkusThill.github.io/images/2254s.jpg" alt="Separating High-Dimensional Data with given Points for the Decision Boundary">
              <div class="image-credit">Image source: <a target="_blank" href="https://www.freepik.com">Designed by jcomp / Freepik</a></div><!-- /.image-credit -->
            </div>
        
      <h1 class="post-title entry-title">Separating High-Dimensional Data with given Points for the Decision Boundary</h1>
      <p>\(
   \def\matr#1{\mathbf #1}
   \def\tp{\mathsf T}
\)</p>

<p>Recently, someone asked me to take a look at a strange problem, which seemed to be quite trivial on the first glance (and actually is, if we put our knowledge of vector and matrix algebra together):
We have quite a few of <script type="math/tex">n</script>-dimensional data-points <script type="math/tex">\vec x_k \in \mathbb{R}^n</script> (exact knowledge of the actual data is not required here), which belong to one of two classes. For a few cases we know the class, but for many cases we do not. This might appear like a typical classification task, if it was not for one additional curiosity: we actually already know quite some data-points which lie on (or very close to) the decision boundary. This turns our original problem into some sort of regression problem, since we can estimate a hypersurface that fits the decision boundary.
You also have to think about several problems in this case as well (e.g., how to incorporate those data-points into the model, which do not lie in the boundary region but for which a class label is known), but in this blog post we want to look at another approach, which in the end provides a linear decision boundary.</p>

<p>The problem is illustrated in the figure below.
<img src="https://MarkusThill.github.io/images/2017-10-06-separating-high-dimensional-data-with-given-points-on-the-cutting-hyperplane/problem.png" alt="Illustration of the Problem" class="image-center" />
As we can see in the figure, there are data points belonging to 2 different classes (red circles and blue crosses), which are nearly linearly separable. Furthermore we also have points, for which the class labels are unknown (black circles). Our task is, to classify these points based on some suitable decision boundary, which yet has to be found. Finally, we can also see some gray stars in the graph. These are the points for which we already know that they are very close to the actual decision boundary. One particular decision boundary is shown here, which passes exactly through 2 of the gray points.</p>

<!--more-->

<p>Since we had more than <script type="math/tex">n</script> of such points, my first idea was to simply find a hyperplane that fits through <script type="math/tex">n</script> of the given points (assuming all are linearly independent) in order to get a simple decision boundary and to evaluate the performance of this simple model. There are several things to do for this:</p>

<ol>
  <li>We have to find the support vector and the direction vectors of the hyperplane (parametric form).</li>
  <li>A relation is required, which can decide on which side of the plane a new (with an unknown class) point is located. The parametric form is not ideal for this purpose, so that an alternative has to be found.</li>
  <li>Ideally, a distance (in this case a simple euclidean distance) to the plane should be computed, which can be an indication for the certainty of a classification decision.</li>
</ol>

<p>So lets get started: In the simplest case one has to find a line which passes through two points <script type="math/tex">P_1</script> and <script type="math/tex">P_2</script> described by the vectors <script type="math/tex">\vec p_1</script> and <script type="math/tex">\vec p_2</script>. This is illustrated in the following figure.</p>

<p><img src="https://MarkusThill.github.io/images/2017-10-06-separating-high-dimensional-data-with-given-points-on-the-cutting-hyperplane/supportvec.png" alt="Support Vector and Direction Vector for the Parametric equation of a plane" class="image-center" width="400px" /></p>

<p>A parametric description for this line can be easily found to be:</p>

<script type="math/tex; mode=display">\vec x = \vec p_1 + t (\vec p_1 - \vec p_2),</script>

<p>where <script type="math/tex">\vec x</script> is any vector which points to the given line.
Accordingly, a plane in three dimensions can be expressed with three points (<script type="math/tex">\vec p_1, \vec p_2, \vec p_3</script>):</p>

<script type="math/tex; mode=display">\vec x = \vec p_1 + t (\vec p_1 - \vec p_2) + s (\vec p_1 - \vec p_2)</script>

<p>More generally, for the <script type="math/tex">n</script>-dimensional case one can write:</p>

<script type="math/tex; mode=display">\begin{equation}
\vec x = \vec p_1 + \sum_{i=2}^n t_i (\vec p_1 - \vec p_i)
\label{eq:parametricPlane}
\end{equation}</script>

<p>The above parametric form is easy to compute, but using it as a decision boundary for classification does not seem to be straight-forward. We have to find a way to figure out, on which side of the hyperplane a point is located. Ideally we would want a decision boundary (which is for example used for logistic regression) in the form of:</p>

<script type="math/tex; mode=display">w_0 + \sum\limits_{i=1}^{n} w_i x_i = 0</script>

<p>where <script type="math/tex">\vec w \in \mathbb{R}^{n+1}</script> represents the weights of the classifier. Normally, these weights are learnt (e.g. with some gradient descent appproach), in this case these weights are already implicitly given by the points, for which we know that these are on the decision boundary. We just have to find a suitable transformation of equation \eqref{eq:parametricPlane}.</p>

<p>For this purpose, let us first find a normal vector <script type="math/tex">\vec n_0</script>, which stands perpendicular on the hyperplane. The reason for this step will become clearer later. Formally, <script type="math/tex">\vec n_0</script> has to be orthogonal to all direction vectors <script type="math/tex">(\vec p_1 - \vec p_i)</script> in equation \eqref{eq:parametricPlane}. One approach (although not always the best for numerical reasons) to find <script type="math/tex">\vec n_0</script> is to compute the cross product of all direction vectors. This is described in another <a href="/vector%20algebra/a-generalization-of-the-vector-cross-product/">blog post</a>.
Typically, the normal vector is normalized so that <script type="math/tex">\lvert \vec n_0 \rvert = 1</script>.</p>

<p>Now let us reformulate equation \eqref{eq:parametricPlane} a little (remember that the normal vector was orthogonal to all direction vectors <script type="math/tex">(\vec p_1 - \vec p_i)</script> on the hyperplane):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\vec x &= \vec p_1 + \sum_{i=2}^n t_i (\vec p_1 - \vec p_i)  \\
\vec x - \vec p_1 &=  \sum_{i=2}^n t_i (\vec p_1 - \vec p_i) \\
\vec n_0 (\vec x - \vec p_1) &=  \vec n_0 \sum_{i=2}^n t_i (\vec p_1 - \vec p_i) \\
\vec n_0 (\vec x - \vec p_1) &=  \vec n_0 \sum_{i=2}^n t_i (\vec p_1 - \vec p_i) \\
\vec n_0 (\vec x - \vec p_1) &=   \sum_{i=2}^n t_i \underbrace{(\vec p_1 - \vec p_i) \vec n_0}_{=0} \\
\vec n_0 (\vec x - \vec p_1) &=   0 \\
\underbrace{\vec n_0 \vec x}_{\sum_{i=1}^n (n_0)_ i x_i } - \vec n_0 \vec p_1 &=   0 \\
\sum_{i=1}^n \underbrace{(n_0)_ i}_{w_i} x_i  \underbrace{- \vec n_0 \vec p_1}_{w_0} &=   0 \label{eq:parametricPlaneReform1} \\
{w_0} + \sum_{i=1}^n {w_i} x_i &=   0 \label{eq:parametricPlaneReform}
\end{align} %]]></script>

<p>This is exactly the equation that we attempted to receive.</p>

<p>For the sake of completeness, let us look at another way to find above equation:
Once the normal vector is known, another algebraic description of the hyperplane can be found in a slightly different way. Only a few simple vector algebra rules are required. The figure below shows a simple illustration for two dimensions.</p>

<p><img src="https://MarkusThill.github.io/images/2017-10-06-separating-high-dimensional-data-with-given-points-on-the-cutting-hyperplane/hessenormal.png" alt="Normal Vector perpendicular to a Hyperplane" class="image-center" width="400px" /></p>

<p>The vector <script type="math/tex">d \cdot \vec n_0</script> (where <script type="math/tex">d</script> is a scalar value) is perpendicular to the given line (or hyperplane in higher dimensions), so that we can find the relation:</p>

<script type="math/tex; mode=display">\begin{equation}
\cos \alpha = \frac{\lvert d \cdot \vec n_0 \rvert}{\lvert \vec x \rvert} = \frac{d}{\lvert \vec x \rvert}
\label{eq:cosAlpha}
\end{equation}</script>

<p>Furthermore, we know that</p>

<script type="math/tex; mode=display">\begin{equation}
\vec n_0^\tp \vec x = \cos \alpha \cdot \lvert \vec n_0 \rvert  \lvert \vec x \rvert = \cos \alpha \cdot \lvert \vec x \rvert
\label{eq:dotProduct}
\end{equation}.</script>

<p>If we now insert \eqref{eq:cosAlpha} into \eqref{eq:dotProduct} we get the simple expression:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\vec n_0^\tp \vec x &= d \label{eq:hesse}\\
\sum_{i=1}^n (n_0)_i x_ i - d &= 0 \\
\sum_{i=1}^n w_i x_ i + w_0 &= 0 \\
 \vec w \begin{pmatrix} 1 \\ \vec x \end{pmatrix} &= 0
\end{align} %]]></script>

<p>You will notice, that equation \eqref{eq:hesse} is actually the same as \eqref{eq:parametricPlaneReform1} – \eqref{eq:parametricPlaneReform}, where <script type="math/tex">w_i = (n_0)_ i</script> and <script type="math/tex">w_0 = -d</script>.</p>

<p>Now not much left is to do. Since we have the decision boundary in a nicer form now, we can find a way to figure out on which side of decision boundary an arbitrary point is located, ideally connected with a distance from the boundary. Let us look at a vector (data-point) <script type="math/tex">\vec r</script> which does not lie on the decision boundary, as shown in the figure below.</p>

<p><img src="https://MarkusThill.github.io/images/2017-10-06-separating-high-dimensional-data-with-given-points-on-the-cutting-hyperplane/hessedistance.png" alt="Normal Vector perpendicular to a Hyperplane" class="image-center" width="400px" /></p>

<p>In this example the vector <script type="math/tex">\vec r</script> has a positive distance <script type="math/tex">d_r</script> from the decision boundary. Hence, we can express <script type="math/tex">\vec r</script> in terms of some vector <script type="math/tex">\vec x</script> – which points to the decision boundary – and the normal vector <script type="math/tex">\vec n_0</script> (assuming for now that we know the actual value of <script type="math/tex">d_r</script>):</p>

<script type="math/tex; mode=display">\begin{equation}
\vec x = \vec r - d_r \vec n_0
\label{eq:newRVec}
\end{equation}</script>

<p>In this case <script type="math/tex">d_r</script> is positive, which means that <script type="math/tex">\vec r</script> is on the side of the decision boundary which is indicated by the direction of <script type="math/tex">\vec n_0</script>. If <script type="math/tex">d_r</script> was negative, <script type="math/tex">\vec r</script> would be located in the other side of the decision boundary. Since <script type="math/tex">\vec x</script> is a vector on the line, we can insert \eqref{eq:newRVec} into \eqref{eq:hesse} and get the following:</p>

<p><script type="math/tex">% <![CDATA[
\begin{align}
\vec n_0 (\vec r - d_r \vec n_0) &= d \\
\vec n_0 \vec r - d_r \vec n_0 \vec n_0 &= d \\
\vec n_0 \vec r - d_r \lvert \vec n_0 \lvert^2 &= d \\
\vec n_0 \vec r - d_r  &= d \\
d_r  &= \vec n_0 \vec r - d  \\
d_r  &=  \sum_{i=1}^n {w_i} r_i + {w_0},
\label{eq:hesseDist}
\end{align} %]]></script>
where a comparison to equations \eqref{eq:parametricPlaneReform1} – \eqref{eq:parametricPlaneReform} and \eqref{eq:hesse} reveals that <script type="math/tex">\vec n_0</script> can be expressed in terms of the weights <script type="math/tex">w_1, \ldots, w_n</script> and <script type="math/tex">w_0 = -d</script>.</p>

<p>With this last equation \eqref{eq:hesseDist} we have found a way to compute the euclidean distance <script type="math/tex">d_r</script> of any point <script type="math/tex">\vec r</script> to the decision boundary. Furthermore, the sign of <script type="math/tex">d_r</script> indicates on which side of the decision boundary a point is located. This is essential for classifying new data-points.</p>

<p>Back to our original problem: The approach described above provides a linear decision boundary when a sufficient number of points on the decision boundary are known (<script type="math/tex">n</script> points when the input-space has <script type="math/tex">n</script> dimensions).
In summary, the following steps have to be performed in order to make it work:</p>
<ol>
  <li>Select exactly <script type="math/tex">n</script> linearly independent data points from the set of points on the decision boundary</li>
  <li>For these points, compute the direction vectors on the hyperplane</li>
  <li>With the direction vectors, compute a normal vector that is orthogonal to all direction vectors</li>
  <li>With the normal vector, determine the weights <script type="math/tex">\vec w \in \mathbb{R}^{n+1}</script> of a linear classifier based on Eq.\eqref{eq:parametricPlaneReform1}</li>
  <li>Classify new points using the model determined in 4.</li>
</ol>

<p>Obviously, this approach does not deliver optimal results in many cases but can be used as some kind of baseline for more advanced algorithms. For our particular problem, it actually worked quite well to describe the decision boundary by a hyperplane which passed through <script type="math/tex">n</script> data-points in the boundary region.</p>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="https://MarkusThill.github.io/tags#Classification" title="Pages tagged Classification" class="tag"><span class="term">Classification</span></a><a href="https://MarkusThill.github.io/tags#Multidimensionality" title="Pages tagged Multidimensionality" class="tag"><span class="term">Multidimensionality</span></a></span>
        
        <span class="author vcard"><span class="fn">Markus Thill</span></span>
        <div class="social-share">
  <ul class="socialcount socialcount-small inline-list">
    <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=https://MarkusThill.github.io/separating-highD-data/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li>
    <li class="twitter"><a href="https://twitter.com/intent/tweet?text=https://MarkusThill.github.io/separating-highD-data/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li>
    <li class="googleplus"><a href="https://plus.google.com/share?url=https://MarkusThill.github.io/separating-highD-data/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li>
  </ul>
</div><!-- /.social-share -->

      </footer>
    </div><!-- /.entry-content -->
    <div class="read-more">
  <div class="read-more-header">
    <a href="https://MarkusThill.github.io/markus" class="read-more-btn">About the Author</a>
  </div><!-- /.read-more-header -->
  <div class="read-more-content author-info">
    <h3>Markus Thill</h3>
    <div class="author-container">
      <img class="author-img" src="https://MarkusThill.github.io/images/avatar.jpg" alt="Markus Thill" />
      <div class="author-bio">I studied computer engineering (B.Sc.) and Automation & IT (M.Eng.). Generally, I am interested in machine learning (ML) approaches (in the broadest sense), but particularly in the fields of time series analysis, anomaly detection, Reinforcement Learning (e.g. for board games), Deep Learning (DL) and incremental (on-line) learning procedures. </div>
    </div>
    <div class="author-share">
      <ul class="list-inline social-buttons">
        
          <li><a href="https://github.com/markusthill" target="_blank"><i class="fa fa-github fa-fw"></i></a></li>
        
          <li><a href="https://www.linkedin.com/in/markus-thill-a4991090/" target="_blank"><i class="fa fa-linkedin fa-fw"></i></a></li>
        
      </ul>
      
        <a aria-label="Follow @MarkusThill on GitHub" data-size="large" href="https://github.com/MarkusThill" class="github-button">Follow @MarkusThill</a>
      
      <br>
      
    </div>
  </div>
</div>

    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="https://MarkusThill.github.io/vector%20algebra/a-generalization-of-the-vector-cross-product/" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="https://MarkusThill.github.io/deriving-a-closed-form-solution-of-the-fibonacci-sequence/" title="Deriving a Closed-Form Solution of the Fibonacci Sequence using the Z-Transform">Deriving a Closed-Form Solution of the Fibonacci Sequence using the Z-Transform</a></h3>
      <p>The Fibonacci sequence might be one of the most famous sequences in the field of mathmatics and computer science. Already high school stu...&hellip; <a href="https://MarkusThill.github.io/deriving-a-closed-form-solution-of-the-fibonacci-sequence/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="https://MarkusThill.github.io/derivation-of-a-weighted-recursive-least-squares-estimator/" title="Derivation of a Weighted Recursive Linear Least Squares Estimator">Derivation of a Weighted Recursive Linear Least Squares Estimator</a></h4>
        <span>Published on May 05, 2019</span>
      </div><!-- /.list-item -->
    
      <div class="list-item">
        <h4><a href="https://MarkusThill.github.io/gaussian-distribution-with-a-diagonal-covariance-matrix/" title="Gaussian Distribution With a Diagonal Covariance Matrix">Gaussian Distribution With a Diagonal Covariance Matrix</a></h4>
        <span>Published on May 04, 2019</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript">window.jQuery || document.write('<script type="text/javascript" src="https://MarkusThill.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script type="text/javascript" src="https://MarkusThill.github.io/assets/js/scripts.min.js"></script>
<script type="text/javascript" async defer id="github-bjs" src="https://buttons.github.io/buttons.js"></script>
<script type="text/javascript">!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^https:/.test(d.location)?'https':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>



<!-- Asynchronous Google Analytics snippet -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-113918188-1', 'auto');
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>



    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'markusthill-github-io'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>




<script type="text/javascript">
    sharing();
</script>



<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 Markus Thill. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://github.com/aron-bordin/neo-hpstr-jekyll-theme" rel="nofollow">Neo-HPSTR Theme</a>.</span>

  </footer>
</div><!-- /.footer-wrapper -->

</body>
</html>
